{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f59ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "''' import necessary libraries'''\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "\n",
    "from utils import create_dataset, get_evaluate_results, EarlyStoppingCallBack, compute_metrics, read_fasta_to_dataframe\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForTokenClassification, TrainingArguments, Trainer,set_seed\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3444ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameter: 1690114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4148083/4264039977.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/home/xie/GraphIDP/model/GCN_LoRA_paramas.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import DisoGraph\n",
    "from layers import modify_with_lora, LoRAConfig\n",
    "# Load pre-trained ESM-2 model and tokenizer\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "model = EsmModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "\n",
    "class_model = DisoGraph(config=model.config)\n",
    "class_model.encoder = model\n",
    "model = class_model\n",
    "del class_model\n",
    "\n",
    "# Freeze ESM model parameters\n",
    "for (param_name, param) in model.encoder.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add LoRA to the model\n",
    "config = LoRAConfig()\n",
    "model = modify_with_lora(model, config)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Trainable Parameter: \"+ str(params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5c22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2954 1229\n"
     ]
    }
   ],
   "source": [
    "# Preprocess inputs\n",
    "train_df = read_fasta_to_dataframe(\"/home/xie/CEFF-IDP/Datasets/DM3000.fasta\")\n",
    "valid_df = read_fasta_to_dataframe(\"/home/xie/CEFF-IDP/Datasets/Validset.fasta\")\n",
    "\n",
    "# Create Datasets\n",
    "train_set=create_dataset(tokenizer,list(train_df['sequence']),list(train_df['label']))\n",
    "valid_set=create_dataset(tokenizer,list(valid_df['sequence']),list(valid_df['label']))\n",
    "\n",
    "print(len(train_df),len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_711455/339790029.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model, args, train_dataset=train_set, eval_dataset=valid_set, tokenizer=tokenizer,\n",
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='988' max='1235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 988/1235 1:01:06 < 15:18, 0.27 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.189760</td>\n",
       "      <td>0.939984</td>\n",
       "      <td>0.625246</td>\n",
       "      <td>0.913754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.184414</td>\n",
       "      <td>0.940646</td>\n",
       "      <td>0.633104</td>\n",
       "      <td>0.919658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.184457</td>\n",
       "      <td>0.941407</td>\n",
       "      <td>0.625607</td>\n",
       "      <td>0.921355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.148700</td>\n",
       "      <td>0.185339</td>\n",
       "      <td>0.941223</td>\n",
       "      <td>0.628557</td>\n",
       "      <td>0.921698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/xie/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=988, training_loss=0.17063367511579383, metrics={'train_runtime': 3683.3462, 'train_samples_per_second': 4.01, 'train_steps_per_second': 0.335, 'total_flos': 0.0, 'train_loss': 0.17063367511579383, 'epoch': 4.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "''' Training Definition '''\n",
    "# # Set random seeds for reproducibility of your trainings run\n",
    "def set_seeds(s):\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    set_seed(s)\n",
    "\n",
    "# Huggingface Trainer arguments\n",
    "args = TrainingArguments(\n",
    "        \"./\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        logging_strategy = \"epoch\",\n",
    "        save_strategy = \"no\",\n",
    "        learning_rate=3e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=5,\n",
    "        deepspeed= None,\n",
    "        fp16 = False,\n",
    "        seed= 42,\n",
    "    ) \n",
    "\n",
    "set_seeds(42)\n",
    "# For token classification we need a data collator here to pad correctly\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, args, train_dataset=train_set, eval_dataset=valid_set, tokenizer=tokenizer,\n",
    "                  data_collator=data_collator, compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallBack(patience=2)])\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
